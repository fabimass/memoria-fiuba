\chapter{Introducción específica} % Main chapter title

\label{Chapter2}

%----------------------------------------------------------------------------------------

Este capítulo tiene como objetivo presentar las técnicas y herramientas clave utilizadas en el 
desarrollo de este trabajo. Se analizan los enfoques fundamentales para el procesamiento de 
lenguaje natural, junto con los modelos, frameworks e infraestructura necesarios para 
construir un sistema de recuperación de información eficiente y escalable. Este recorrido 
técnico permite comprender los fundamentos sobre los que se apoya la solución implementada. 

%---------------------------------------------------------------------------------------
\section{Técnicas de procesamiento de lenguaje natural}

El procesamiento de lenguaje natural (NLP) es un área de la inteligencia artificial que permite 
a las máquinas comprender e interpretar el lenguaje humano \citep{book:nlp}. Esto juega un papel esencial 
para que el chatbot interprete las consultas de los usuarios y localice la información relevante en los 
documentos procesados. Las técnicas de NLP aplicadas permiten que el sistema entienda el 
significado de las consultas, independientemente de variaciones lingüísticas o de sintaxis. Entre los 
métodos de mayor importancia para este trabajo se encuentran los \textit{word embeddings} y
\textit{transformers}.

\subsection{Word embeddings}

Los \textit{embeddings} son una poderosa técnica para transformar datos complejos en formas numéricas que 
pueden ser fácilmente procesadas y analizadas por algoritmos de aprendizaje automático. Esta técnica permite 
representar virtualmente cualquier tipo de dato como vectores, lo que posibilita su manipulación en tareas de 
procesamiento de lenguaje natural.

Sin embargo, no se trata solo de convertir las palabras en vectores. Es crucial preservar el significado original 
de los datos para que las tareas realizadas en este espacio transformado mantengan la coherencia semántica. 
Por ejemplo, al comparar dos frases, no queremos simplemente comparar las palabras que contienen, sino evaluar 
si ambas expresan un significado similar.

Para conservar el significado, es necesario generar vectores donde las relaciones entre ellos sean representativas 
del contenido. Para ello, se emplea un modelo de \textit{embeddings} pre-entrenado, que produce una representación compacta 
de los datos mientras mantiene sus características semánticas. El objetivo es capturar el significado o las relaciones 
semánticas entre los puntos de datos, de modo que los elementos similares se encuentren cercanos en el espacio vectorial y los disímiles estén alejados. 
Por ejemplo, consideremos las palabras ``rey'' y ``reina''. Un \textit{embedding} puede mapear estas palabras en vectores de modo 
que la diferencia entre ``rey' y ``reina'' sea similar a la diferencia entre ``hombre'' y ``mujer'', reflejando así las 
relaciones semánticas subyacentes.

En este trabajo, se ensayaron diferentes modelos de \textit{embeddings} de OpenAI, Google y Hugging Face, 
cuya evaluación y selección se detalla en el capítulo \ref{Chapter4}.

\subsection{Transformers}

La arquitectura de \textit{transformers} ha revolucionado el procesamiento de lenguaje natural al introducir un mecanismo de \textit{self-attention}, 
que permite a un modelo evaluar la relevancia de cada palabra en una secuencia en relación con las demás \citep{paper:transformers}. Este enfoque 
supera las limitaciones de modelos secuenciales tradicionales, ya que permite procesar palabras en paralelo y captar dependencias de largo alcance 
en el texto. En lugar de analizar cada palabra en un orden específico, el mecanismo de \textit{self-attention} permite que el modelo ``preste atención'' 
a las palabras más relevantes en el contexto de la frase, asignando pesos a cada palabra según su importancia relativa en la oración.

Gracias a esta capacidad, los \textit{transformers} pueden capturar relaciones contextuales complejas y matices semánticos entre las palabras, 
lo que resulta fundamental para tareas como la generación de texto. Esta arquitectura ha hecho posible que los modelos comprendan y generen 
lenguaje natural de una forma mucho más cercana a la comprensión humana, permitiendo respuestas precisas y contextualmente adecuadas. Esta técnica 
ha sido fundamental en el desarrollo de los modelos grandes de lenguaje, los cuales se presentan en la siguiente sección.

%----------------------------------------------------------------------------------------
\section{Modelos grandes de lenguaje}

Los modelos grandes de lenguaje (LLM, por sus siglas en inglés) son redes neuronales de gran escala, entrenadas 
para comprender y generar texto en lenguaje natural. Estos modelos, basados en arquitecturas de \textit{transformers}, 
están compuestos por millones o incluso billones de parámetros, lo que les permite capturar patrones complejos y 
relaciones contextuales en vastos conjuntos de datos de texto. Los LLM son capaces de realizar múltiples 
tareas de procesamiento de lenguaje natural, como la generación de texto, la traducción 
automática y el resumen de documentos. En la tabla \ref{tab:llms} se presentan 
algunos de los modelos más relevantes en la actualidad.

\begin{table}[h]
	\centering
	\caption[Modelos LLM disponibles en el mercado]{Modelos LLM disponibles en el mercado}
	\begin{tabular}{l c c c}    
		\toprule
		\textbf{Modelo} 	 & \textbf{Creador} 	& \textbf{Año de publicación}  & \textbf{Cant. de parámetros (aprox.)}\\
		\midrule
		GPT-4o               & OpenAI 				& 2024                         & No divulgado\\		
		Gemini 1.5      	 & Google				& 2024                         & No divulgado\\
		LLaMa 3         	 & Meta				    & 2024                         & 70 mil millones\\
        GPT-4         	     & OpenAI				& 2023                         & 1 billón\\
        LLaMa 2         	 & Meta				    & 2023                         & 13 mil millones\\
        Mistral-7B         	 & Mistral AI		    & 2023                         & 7 mil millones\\
        BLOOM         	     & BigScience		    & 2022                         & 176 mil millones\\
        GPT-3.5         	 & OpenAI				& 2022                         & 175 mil millones\\
		\bottomrule
		\hline
	\end{tabular}
	\label{tab:llms}
\end{table}

En este trabajo, el modelo LLM desempeña un papel central ya que es el encargado de entender las consultas de 
los usuarios y generar respuestas coherentes. Para seleccionar el modelo más adecuado, 
se ensayaron diferentes variantes, cuyos detalles se describen en el capítulo \ref{Chapter4}.

%----------------------------------------------------------------------------------------
\section{Generación aumentada por recuperación}

%----------------------------------------------------------------------------------------
\section{Frameworks utilizados}

%----------------------------------------------------------------------------------------
\section{Bases de datos vectoriales}

%----------------------------------------------------------------------------------------
\section{Servicios en la nube}